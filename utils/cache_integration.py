import hashlib

#!/usr/bin/env python3
"""
Cache Integration Layer - Wires EnhancedCacheManager into prediction service
Provides backward-compatible interface while adding multi-tier caching capabilities

NOW WITH SWR (Stale-While-Revalidate):
- Serves cached predictions instantly, even if slightly stale
- Refreshes stale entries in background for next request
- Reduces prediction latency from ~300ms to <10ms for cached entries
"""

import asyncio
import logging
import os
import sys
from pathlib import Path
from typing import Any, Callable, Dict, Optional, Tuple

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.enhanced_cache_manager import EnhancedCacheManager, get_cache_manager

logger = logging.getLogger(__name__)


class PredictionCacheAdapter:
    """
    Adapter to make EnhancedCacheManager compatible with existing PredictionCache interface.
    
    This maintains backward compatibility while adding:
    - L1 (memory) â†’ L2 (Redis) â†’ L3 (disk) fallback
    - Stale-while-revalidate for non-blocking updates
    - Better metrics and promotion logic
    
    SWR Benefits:
    - Instant response for cached predictions (even if slightly stale)
    - Background refresh ensures next request gets fresh data
    - Dramatically reduced latency: ~300ms â†’ <10ms for cache hits
    """
    
    def __init__(
        self,
        max_entries: int = 1000,
        ttl: int = 3600,
        enable_redis: bool = None,
        redis_url: Optional[str] = None,
        enable_swr: bool = True
    ):
        """
        Initialize cache adapter.
        
        Args:
            max_entries: Max L1 memory cache entries
            ttl: Default time-to-live in seconds
            enable_redis: Enable L2 Redis (auto-detect from env if None)
            redis_url: Redis connection URL (defaults to REDIS_URL env var)
            enable_swr: Enable stale-while-revalidate (default: True)
        """
        # Auto-detect Redis from environment
        if enable_redis is None:
            enable_redis = bool(os.getenv('REDIS_URL') or os.getenv('REDIS_ENABLED'))
        
        if redis_url is None:
            redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379/0')
        
        # SWR configuration
        self._enable_swr = enable_swr
        
        # Initialize enhanced cache manager
        self._cache = get_cache_manager(
            l1_max_size=max_entries,
            l2_enabled=enable_redis,
            l2_redis_url=redis_url if enable_redis else None,
            l3_cache_dir=Path("data/cache/predictions"),
            default_ttl=ttl,
            stale_ttl=ttl * 2,  # Allow serving stale for 2x TTL
            enable_metrics=True
        )
        
        # Track data timestamp for invalidation
        self._last_data_timestamp: Optional[str] = None
        
        # CONSISTENCY FIX: Track data content hash instead of just timestamp
        self._data_content_hash: Optional[str] = None
        self._prediction_hashes: Dict[str, str] = {}  # Track hash per prediction
        
        logger.info(
            f"ðŸ“¦ PredictionCacheAdapter initialized: "
            f"max_entries={max_entries}, ttl={ttl}s, redis={enable_redis}, swr={enable_swr}"
        )
    
    def get(self, key: Tuple[str, str, str]) -> Optional[Dict[str, Any]]:
        """
        Get prediction from cache.
        
        Args:
            key: (home_team, away_team, league_code) tuple
        
        Returns:
            Cached prediction or None
        """
        # Convert tuple key to string for cache manager
        cache_key = self._tuple_to_key(key)
        
        # Check if data timestamp changed (invalidation signal)
        try:
            from prediction_ui.metrics import get_data_fresh_timestamp
            current_data_ts = get_data_fresh_timestamp()
            
            # CONSISTENCY FIX: Content-based invalidation instead of timestamp
            if current_data_ts:
                # Compute content hash from actual data
                try:
                    from prediction_ui.metrics import get_data_content_hash
                    current_hash = get_data_content_hash()
                except ImportError:
                    # Fallback to timestamp-based with longer grace period
                    current_hash = str(current_data_ts)
                
                if self._data_content_hash != current_hash and self._data_content_hash is not None:
                    logger.info(
                        f"[CACHE] Data content changed (hash: {self._data_content_hash[:8]} â†’ {current_hash[:8]}), "
                        f"invalidating affected predictions"
                    )
                    # Only clear if hash actually changed (content changed)
                    self._cache.clear(pattern="pred:")
                    self._prediction_hashes.clear()
                
                self._data_content_hash = current_hash
                self._last_data_timestamp = current_data_ts
        except ImportError:
            # Metrics not available, skip invalidation check
            pass
        
        # Retrieve from cache
        result = self._cache.get(cache_key)
        return result
    
    def set(self, key: Tuple[str, str, str], value: Dict[str, Any]):
        """
        Store prediction in cache.
        
        Args:
            key: (home_team, away_team, league_code) tuple
            value: Prediction data
        """
        # Update data timestamp tracker
        try:
            from prediction_ui.metrics import get_data_fresh_timestamp
            current_data_ts = get_data_fresh_timestamp()
            if current_data_ts:
                self._last_data_timestamp = current_data_ts
        except ImportError:
            pass
        
        # Convert tuple key to string
        cache_key = self._tuple_to_key(key)
        
        # Store in cache (will propagate to L1/L2/L3)
        self._cache.set(cache_key, value)
    
    def get_or_predict(
        self,
        key: Tuple[str, str, str],
        predict_func: Callable[[], Dict[str, Any]],
        ttl: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Get prediction from cache or generate if missing (SWR-enabled).
        
        This is the recommended method for predictions as it:
        - Serves cached predictions instantly (even if slightly stale)
        - Refreshes stale predictions in background
        - Falls back to fresh prediction on cache miss
        
        Args:
            key: (home_team, away_team, league_code) tuple
            predict_func: Function to generate fresh prediction
            ttl: Optional TTL override
        
        Returns:
            Prediction dict
        """
        cache_key = self._tuple_to_key(key)
        
        # Use SWR if enabled
        if self._enable_swr:
            try:
                result = self._cache.get_or_fetch(
                    cache_key,
                    predict_func,
                    ttl=ttl,
                    stale_while_revalidate=True
                )
                return result
            except Exception as e:
                logger.warning(f"[SWR] get_or_fetch failed for {cache_key}: {e}, falling back to direct prediction")
                # Fallback to direct prediction
                return predict_func()
        else:
            # SWR disabled, use traditional cache-aside pattern
            cached = self.get(key)
            if cached is not None:
                return cached
            
            # Cache miss - generate fresh
            result = predict_func()
            self.set(key, result)
            return result
    
    def stats(self) -> Dict[str, Any]:
        """
        Get cache statistics.
        
        Returns:
            Dict with hits, misses, hit_rate, SWR stats, etc.
        """
        metrics = self._cache.get_metrics()
        
        # Convert to legacy format for backward compatibility
        total_requests = metrics.get('total_requests', 0)
        hits = metrics.get('l1_hits', 0) + metrics.get('l2_hits', 0) + metrics.get('l3_hits', 0)
        misses = metrics.get('misses', 0)
        
        return {
            "entries": metrics.get('l1_size', 0),
            "hit_rate": metrics.get('hit_rate', 0.0),
            "hits": hits,
            "misses": misses,
            "evictions": metrics.get('evictions', 0),
            "capacity": self._cache._l1_max_size,
            # Enhanced metrics
            "l1_hit_rate": metrics.get('l1_hit_rate', 0.0),
            "l2_hit_rate": metrics.get('l2_hit_rate', 0.0),
            "l3_hit_rate": metrics.get('l3_hit_rate', 0.0),
            "promotions": metrics.get('promotions', 0),
            # SWR-specific metrics
            "stale_served": metrics.get('stale_served', 0),
            "background_refreshes": metrics.get('background_refreshes', 0),
            "swr_enabled": self._enable_swr,
        }
    
    def clear(self):
        """Clear all cache entries."""
        self._cache.clear()
        logger.info("[CACHE] All entries cleared")
    
    def _tuple_to_key(self, key: Tuple[str, str, str]) -> str:
        """Convert tuple key to string format for cache manager."""
        home, away, league = key
        return f"pred:{league}:{home}_vs_{away}"
    
    def print_metrics(self):
        """Print formatted cache metrics."""
        self._cache.print_metrics()


def create_prediction_cache(
    max_entries: int = 1000,
    ttl: int = 3600,
    enable_redis: bool = None,
    redis_url: Optional[str] = None,
    enable_swr: bool = True
) -> PredictionCacheAdapter:
    """
    Factory function to create prediction cache.
    
    Args:
        max_entries: Max L1 entries
        ttl: Time-to-live in seconds
        enable_redis: Enable L2 Redis cache
        redis_url: Redis connection URL
        enable_swr: Enable stale-while-revalidate (recommended: True)
    
    Returns:
        Configured PredictionCacheAdapter with SWR support
    """
    return PredictionCacheAdapter(
        max_entries=max_entries,
        ttl=ttl,
        enable_redis=enable_redis,
        redis_url=redis_url,
        enable_swr=enable_swr
    )


if __name__ == "__main__":
    # Test the adapter with SWR
    logging.basicConfig(level=logging.INFO)
    
    print("\n=== Testing PredictionCacheAdapter with SWR ===\n")
    
    # Create adapter with SWR enabled
    cache = create_prediction_cache(max_entries=100, ttl=5, enable_redis=False, enable_swr=True)
    
    # Test traditional operations
    test_key = ("Arsenal", "Chelsea", "PL")
    test_value = {
        "home_win_prob": 0.45,
        "draw_prob": 0.30,
        "away_win_prob": 0.25,
        "confidence": 0.82
    }
    
    # Set and get
    cache.set(test_key, test_value)
    retrieved = cache.get(test_key)
    print(f"âœ… Set and retrieved: {retrieved}")
    
    # Test SWR get_or_predict
    import time
    
    class MockPredictor:
        def __init__(self):
            self.call_count = 0
        
        def predict(self):
            self.call_count += 1
            print(f"  ðŸ”® Generating prediction #{self.call_count}")
            return {
                "home_win_prob": 0.50,
                "draw_prob": 0.25,
                "away_win_prob": 0.25,
                "confidence": 0.85,
                "timestamp": time.time()
            }
    
    mock_predictor = MockPredictor()
    
    # First call - cache miss, should generate
    print("\n--- First call (cache miss) ---")
    result1 = cache.get_or_predict(("ManCity", "Liverpool", "PL"), mock_predictor.predict, ttl=3)
    print(f"Result: confidence={result1['confidence']}")
    print(f"Prediction calls so far: {mock_predictor.call_count}")
    
    # Second call - cache hit, should NOT generate
    print("\n--- Second call (cache hit) ---")
    result2 = cache.get_or_predict(("ManCity", "Liverpool", "PL"), mock_predictor.predict, ttl=3)
    print(f"Result: confidence={result2['confidence']}")
    print(f"Prediction calls so far: {mock_predictor.call_count}")
    
    # Wait for TTL to expire
    print("\n--- Waiting 4 seconds for cache to become stale ---")
    time.sleep(4)
    
    # Third call - stale data, should serve stale + trigger background refresh
    print("\n--- Third call (stale, SWR active) ---")
    result3 = cache.get_or_predict(("ManCity", "Liverpool", "PL"), mock_predictor.predict, ttl=3)
    print(f"Result: confidence={result3['confidence']} (served instantly from stale cache)")
    print(f"Prediction calls so far: {mock_predictor.call_count} (background refresh may be pending)")
    
    # Give background refresh time to complete
    time.sleep(0.5)
    print(f"After background refresh: {mock_predictor.call_count} calls total")
    
    # Stats
    stats = cache.stats()
    print(f"\nðŸ“Š Cache Stats:")
    print(f"  Hit Rate: {stats['hit_rate']:.1%}")
    print(f"  Hits: {stats['hits']}, Misses: {stats['misses']}")
    print(f"  Stale Served: {stats['stale_served']}")
    print(f"  Background Refreshes: {stats['background_refreshes']}")
    print(f"  SWR Enabled: {stats['swr_enabled']}")
    
    # Detailed metrics
    print(f"\nðŸ“ˆ Enhanced Metrics:")
    cache.print_metrics()
